{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Users/tylerfolkman/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Most of this code is borrowed from: https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "'''\n",
    "\n",
    "import theano\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.datasets.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131792\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"candidate_words_dict.json\") as f:\n",
    "    candidate_word_dict = json.load(f)\n",
    "text = []\n",
    "start_indexes = []\n",
    "i = 0\n",
    "for v in candidate_word_dict.values():\n",
    "    text.append(\"<START>\")\n",
    "    start_indexes.append(i)\n",
    "    i+=1\n",
    "    for word in v:\n",
    "        if word[-1] == '.':\n",
    "            text.append(word[:-1].lower())\n",
    "            text.append(\"<END>\")\n",
    "            text.append(\"<START>\")\n",
    "            i+=3\n",
    "            start_indexes.append(i-1)\n",
    "        else:\n",
    "            text.append(word.lower())\n",
    "            i+=1\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 9198\n"
     ]
    }
   ],
   "source": [
    "## Get unique characters and create dictionaries from character to index and index to character\n",
    "\n",
    "chars = set(text)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 43929\n"
     ]
    }
   ],
   "source": [
    "maxlen = 5\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "## i goes to len(text) - maxlen because grabbing in those sets\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    # get the first set of max len, next time around move start forward step and get max len chars\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    # character immediately following the sentences set\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "# row for each sentence, column for each word in sentence, depth for possible characters\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "# row for each sentence prediction, column for each possible character\n",
    "y = np.zeros((len(next_chars), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        # char indicies return index for a given character\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "\n",
    "# once build model and have some weights, can re-use\n",
    "\n",
    "#model = model_from_json(open('candidate_model_architecture.json').read())\n",
    "#model.load_weights('candidate_lstm_weights.h5')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('candidate_model_architecture.json', 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    # take log of probabilities\n",
    "    a = np.log(a) / temperature\n",
    "    # convert back to probabilities\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    # Sample one given the probabilities and get that index\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "43929/43929 [==============================] - 605s - loss: 4.9875   \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "43929/43929 [==============================] - 2801s - loss: 4.9129  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "43929/43929 [==============================] - 644s - loss: 4.8380   \n"
     ]
    }
   ],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 4):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=64, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('candidate_lstm_weights_v2.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_index = random.choice(start_indexes)\n",
    "\n",
    "for diversity in [.1, .5, .75, .9, 1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = []\n",
    "    # get a random sentence of correct length for model\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated.extend(sentence)\n",
    "    print('----- Generating with seed: {}\\n'.format(u\" \".join(sentence).encode('utf-8').strip()))\n",
    "\n",
    "    next_char = \"\"\n",
    "    while next_char != \"<END>\":\n",
    "        # convert the sentence into the x format necessary for model\n",
    "        # meaning # sentences, # characters in sentence, # vocab\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        # generated keeps track of text im generating\n",
    "        generated.append(next_char)\n",
    "        # sentence shifts down by 1 for next loop\n",
    "        sentence = sentence[1:]\n",
    "        sentence.append(next_char)\n",
    "\n",
    "    print(\"Resulting Setence: {}\".format(u\" \".join(generated).encode('utf-8').strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
